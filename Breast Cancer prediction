import numpy as np
import pandas as pd
import seaborn as s
from sklearn import model_selection
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report,accuracy_score
from sklearn import svm
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from xgboost import XGBClassifier
import os
import warnings
import pickle
warnings.filterwarnings('ignore')
os.chdir ('C:\\2022CH11447\\')
data = pd.read_csv('data.csv')
df = data
df['diagnosis'] = df['diagnosis']. astype('category')
df['diagnosis'] = df['diagnosis'].cat.codes
x= df.drop ('diagnosis',axis =1).drop('id',axis =1)
y= df['diagnosis']
''' Checking for different models to find out the most efficient model with max. accuracy
'''
def FitModel (X,Y, algo_name , algorithm, gridSearchParams, cv):
    np.random.seed(10)
    x_train, x_test, y_train, y_test = train_test_split (X,Y,test_size = 0.2)
# Now finding the Parameters , then choosing the best parameters
    grid = GridSearchCV(estimator = algorithm, param_grid = gridSearchParams,cv = cv, scoring = 'accuracy', verbose = 1 , n_jobs = -1 )
    grid_result = grid.fit(x_train, y_train)
    best_params = grid_result.best_params_
    pred = grid_result.predict (x_test)
    cm = confusion_matrix (y_test,pred)
    pickle.dump(grid_result,open(algo_name,'wb'))
    print (pred)
    print ('Best Params :', best_params)
    print ('Classification Report:',classification_report(y_test,pred))
    print ('Accuracy Score', (accuracy_score(y_test,pred)))
    print ('Confusion Matrix :\n',cm)

# Creating a SVM Model
param = {
'C': [0.1,1,100,1000],
'gamma':[0.0001,0.001, 0.005, 0.1,1, 3,5,10, 100]
}
FitModel (x,y,'SVC',SVC(), param, cv =10)

# Creating a Random Forest Model
param = { 'n_estimators': [100,500,1000,2000] }
FitModel (x,y,'Random Forest',RandomForestClassifier(), param, cv =10)

# Creating XG Boost Model
param = { 'n_estimators': [100,500,1000,2000] }
FitModel (x,y,'XGBoost', XGBClassifier(),param, cv = 10)

# It's clear from the above models that the random forest model is the most efficient one, with parameters param =1000

